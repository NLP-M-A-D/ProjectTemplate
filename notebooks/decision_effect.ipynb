{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46de437",
   "metadata": {},
   "source": [
    "## 0. Setup & Configuration\n",
    "\n",
    "- Fill the `CONFIG` paths for your two books (plain text).\n",
    "- Toggle stopwords and thresholds as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac148dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      9\u001b[39m plt.rcParams[\u001b[33m\"\u001b[39m\u001b[33mfigure.figsize\u001b[39m\u001b[33m\"\u001b[39m] = (\u001b[32m9\u001b[39m, \u001b[32m4.5\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# ===== Imports & Config =====\n",
    "import re, os, math, json, collections\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (9, 4.5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "CONFIG = {\n",
    "    \"book1_path\": \"../data/Crime-punishment.txt\",  # <-- change\n",
    "    \"book2_path\": \"../data/The-Brotherskaramazov.txt\",  # <-- change\n",
    "    \"language\": \"en\",                # e.g. 'en','de','ru','el'\n",
    "    \"use_stopwords\": False,          # toggle\n",
    "    \"min_ngram_count\": 5,            # threshold (where applicable)\n",
    "    \"top_k\": 20                      # top items to show\n",
    "}\n",
    "\n",
    "# Unicode-aware token regex: words with optional internal ' or -\n",
    "WORD_RE = re.compile(r\"[^\\W\\d_]+(?:[-'][^\\W\\d_]+)*\", flags=re.UNICODE)\n",
    "\n",
    "# Optional: supply your own stopwords set per language\n",
    "STOPWORDS = set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f746ea",
   "metadata": {},
   "source": [
    "## 1. Load & Normalize Text\n",
    "\n",
    "- Fix hyphenated line breaks (e.g., end-of-line hyphens).\n",
    "- Normalize whitespace.\n",
    "- Lowercase consistently.\n",
    "\n",
    "Our books are a part of Project Gutenberg, which means there are some extra texts in each txt file to be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cece245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Robust Project Gutenberg boilerplate stripper --------------------------\n",
    "_GB_START_MARKERS = [\n",
    "    r\"\\*\\*\\*\\s*START OF (THIS|THE) PROJECT GUTENBERG EBOOK\",   # modern\n",
    "    r\"START OF (THIS|THE) PROJECT GUTENBERG EBOOK\",             # fallback\n",
    "    r\"End of the Project Gutenberg(?:'s)? Etext\",               # very old variants sometimes inverted\n",
    "]\n",
    "_GB_END_MARKERS = [\n",
    "    r\"\\*\\*\\*\\s*END OF (THIS|THE) PROJECT GUTENBERG EBOOK\",      # modern\n",
    "    r\"END OF (THIS|THE) PROJECT GUTENBERG EBOOK\",                # fallback\n",
    "    r\"End of Project Gutenberg(?:'s)? (?:Etext|eBook)\",          # older variants\n",
    "    r\"\\*\\*\\*\\s*END: FULL LICENSE\\s*\\*\\*\\*\",                      # license block end (older)\n",
    "]\n",
    "\n",
    "# Chapters (heuristic fallback if markers missing; English-centric but works often)\n",
    "_CHAPTER_HINTS = [\n",
    "    r\"^\\s*chapter\\s+[ivxlcdm0-9]+[\\.\\: ]\",   # CHAPTER I / Chapter 1\n",
    "    r\"^\\s*book\\s+[ivxlcdm0-9]+[\\.\\: ]\",      # BOOK I etc.\n",
    "    r\"^\\s*part\\s+[ivxlcdm0-9]+[\\.\\: ]\",\n",
    "]\n",
    "\n",
    "def strip_gutenberg(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns text between Gutenberg START and END markers (case-insensitive).\n",
    "    If markers aren't found, heuristically trims to first chapter-like heading.\n",
    "    Works for most EN/DE/RU/EL releases since headers are in English.\n",
    "    \"\"\"\n",
    "    t = text.replace(\"\\ufeff\", \"\")  # strip BOM if present\n",
    "\n",
    "    # Find START\n",
    "    start_idx = None\n",
    "    for pat in _GB_START_MARKERS:\n",
    "        m = re.search(pat, t, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            # start AFTER the matched line\n",
    "            start_idx = t.find(\"\\n\", m.end())\n",
    "            if start_idx == -1:\n",
    "                start_idx = m.end()\n",
    "            break\n",
    "\n",
    "    # Find END\n",
    "    end_idx = None\n",
    "    for pat in _GB_END_MARKERS:\n",
    "        m = re.search(pat, t, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            # end BEFORE the matched line\n",
    "            end_idx = m.start()\n",
    "            break\n",
    "\n",
    "    if start_idx is not None and end_idx is not None and end_idx > start_idx:\n",
    "        core = t[start_idx:end_idx]\n",
    "    else:\n",
    "        # Fallback: try to start at first chapter-like heading\n",
    "        core = t\n",
    "        for pat in _CHAPTER_HINTS:\n",
    "            m = re.search(pat, core, flags=re.IGNORECASE | re.MULTILINE)\n",
    "            if m:\n",
    "                core = core[m.start():]\n",
    "                break\n",
    "        # And trim off the standard license tail if present\n",
    "        for pat in _GB_END_MARKERS:\n",
    "            m = re.search(pat, core, flags=re.IGNORECASE)\n",
    "            if m:\n",
    "                core = core[:m.start()]\n",
    "                break\n",
    "\n",
    "    # Remove license/contact blocks that sometimes sneak inside\n",
    "    core = re.sub(r\"\\n\\s*End of the Project Gutenberg.*\", \"\", core, flags=re.IGNORECASE)\n",
    "    core = re.sub(r\"\\*\\*\\*\\s*START: FULL LICENSE\\s*\\*\\*\\*.*\", \"\", core, flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    # Clean leftover cruft: URLs, repeated separators\n",
    "    core = re.sub(r\"https?://\\S+\", \"\", core)\n",
    "    core = re.sub(r\"[ \\t]+\\n\", \"\\n\", core)   # trailing spaces before newline\n",
    "    core = re.sub(r\"\\n{3,}\", \"\\n\\n\", core)   # collapse big blank blocks\n",
    "    return core.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6370c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(p: str) -> str:\n",
    "    with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    # 1) strip Gutenberg header/footer FIRST\n",
    "    t = strip_gutenberg(t)\n",
    "    # 2) join hyphenated line breaks (e.g., \"won-\\nderful\")\n",
    "    t = re.sub(r\"-\\s*\\n\", \"\", t)\n",
    "    # 3) normalize whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n",
    "\n",
    "text1 = normalize_text(load_text(CONFIG[\"book1_path\"]))\n",
    "text2 = normalize_text(load_text(CONFIG[\"book2_path\"]))\n",
    "\n",
    "tokens1 = WORD_RE.findall(text1.lower())\n",
    "tokens2 = WORD_RE.findall(text2.lower())\n",
    "\n",
    "if CONFIG[\"use_stopwords\"]:\n",
    "    tokens1 = [t for t in tokens1 if t not in STOPWORDS]\n",
    "    tokens2 = [t for t in tokens2 if t not in STOPWORDS]\n",
    "\n",
    "tokens = tokens1 + tokens2\n",
    "\n",
    "len(tokens1), len(tokens2), len(tokens)\n",
    "\n",
    "len(tokens), tokens[:12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0682b1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dimas\\NLP_w2\\ProjectTemplate\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Crime_punishment.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     30\u001b[39m text2 = normalize_text(load_text(CONFIG[\u001b[33m\"\u001b[39m\u001b[33mbook2_path\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Load character data from JSON files\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Assuming JSON structure like: {\"main_char\": \"Rodion_Raskolnikov\"} for Crime and Punishment\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# And {\"main_chars\": [\"Dmitri_Karamazov\", \"Ivan_Karamazov\", \"Alexei_Karamazov\"]} for Brothers Karamazov\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCrime_punishment.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     36\u001b[39m     char_data1 = json.load(f)\n\u001b[32m     37\u001b[39m     main_char1 = char_data1.get(\u001b[33m'\u001b[39m\u001b[33mmain_char\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mRodion_Raskolnikov\u001b[39m\u001b[33m'\u001b[39m).lower()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dimas\\NLP_w2\\ProjectTemplate\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:344\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    339\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Crime_punishment.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Assuming load_text and normalize_text functions (based on common implementations)\n",
    "def load_text(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Your provided setup\n",
    "CONFIG = {\n",
    "    \"book1_path\": \"../data/Crime-punishment.txt\",  # <-- change\n",
    "    \"book2_path\": \"../data/The-Brotherskaramazov.txt\",  # <-- change\n",
    "    \"language\": \"en\",                # e.g. 'en','de','ru','el'\n",
    "    \"use_stopwords\": False,          # toggle\n",
    "    \"min_ngram_count\": 5,            # threshold (where applicable)\n",
    "    \"top_k\": 20                      # top items to show\n",
    "}\n",
    "\n",
    "text1 = normalize_text(load_text(CONFIG[\"book1_path\"]))\n",
    "text2 = normalize_text(load_text(CONFIG[\"book2_path\"]))\n",
    "\n",
    "# Load character data from JSON files\n",
    "# Assuming JSON structure like: {\"main_char\": \"Rodion_Raskolnikov\"} for Crime and Punishment\n",
    "# And {\"main_chars\": [\"Dmitri_Karamazov\", \"Ivan_Karamazov\", \"Alexei_Karamazov\"]} for Brothers Karamazov\n",
    "with open('Crime_punishment.json', 'r') as f:\n",
    "    char_data1 = json.load(f)\n",
    "    main_char1 = char_data1.get('main_char', 'Rodion_Raskolnikov').lower()\n",
    "\n",
    "with open('The_brothers.json', 'r') as f:\n",
    "    char_data2 = json.load(f)\n",
    "    main_chars2 = [char.lower() for char in char_data2.get('main_chars', [\"Dmitri_Karamazov\", \"Ivan_Karamazov\", \"Alexei_Karamazov\"])]\n",
    "\n",
    "# Define keywords related to decisions and murder/story elements (customize as needed based on stories)\n",
    "decision_keywords = ['decide', 'decision', 'chose', 'choice', 'plan', 'intend', 'resolve']\n",
    "murder_keywords = ['murder', 'kill', 'crime', 'death', 'guilt', 'confess', 'punish', 'trial', 'accuse']\n",
    "\n",
    "# Initialize sentiment analysis pipeline from transformers\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Function to analyze sentiment for a book and its characters\n",
    "def analyze_book_sentiment(text, characters, book_title):\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentiment_scores = []\n",
    "    positions = []  # To track position in the book (sentence index)\n",
    "\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        sentence_lower = sentence.lower()\n",
    "        # Check if any character is mentioned\n",
    "        char_mentioned = any(char in sentence_lower for char in characters)\n",
    "        # Check for decision or murder-related keywords\n",
    "        keyword_match = any(kw in sentence_lower for kw in decision_keywords + murder_keywords)\n",
    "        \n",
    "        if char_mentioned and keyword_match:\n",
    "            # Perform sentiment analysis\n",
    "            result = sentiment_pipeline(sentence)[0]\n",
    "            label = result['label']\n",
    "            score = result['score']\n",
    "            # Map to positive/negative value: positive up, negative down\n",
    "            sentiment_value = score if label == 'POSITIVE' else -score\n",
    "            sentiment_scores.append(sentiment_value)\n",
    "            positions.append(idx)\n",
    "\n",
    "    # Plot the graph\n",
    "    if sentiment_scores:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(positions, sentiment_scores, marker='o', linestyle='-', color='b')\n",
    "        plt.title(f'Sentiment Trajectory for {book_title} - Character Decisions (Murder/Story Related)')\n",
    "        plt.xlabel('Position in Book (Sentence Index)')\n",
    "        plt.ylabel('Sentiment Score (Positive: Up, Negative: Down)')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No relevant decision points found for {book_title}.\")\n",
    "\n",
    "# Analyze Crime and Punishment (single main character)\n",
    "analyze_book_sentiment(text1, [main_char1], \"Crime and Punishment\")\n",
    "\n",
    "# Analyze The Brothers Karamazov (multiple main characters)\n",
    "analyze_book_sentiment(text2, main_chars2, \"The Brothers Karamazov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "077d9653",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './notebooks/Character Library/Crime_punishment.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 313\u001b[39m\n\u001b[32m    310\u001b[39m     export_logs(\u001b[33m\"\u001b[39m\u001b[33mbrothers_karamazov\u001b[39m\u001b[33m\"\u001b[39m, decisions_bros)\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 270\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    267\u001b[39m text2 = normalize_text(load_text(CONFIG[\u001b[33m\"\u001b[39m\u001b[33mbook2_path\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# Load character JSONs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m crime_char_json = \u001b[43mload_character_file\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./notebooks/Character Library/Crime_punishment.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m bros_char_json = load_character_file(\u001b[33m\"\u001b[39m\u001b[33m./notebooks/Character Library/The_brothers.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    273\u001b[39m crime_aliases = build_aliases(crime_char_json)   \u001b[38;5;66;03m# expects Rodion_Raskolnikov aliases\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mload_character_file\u001b[39m\u001b[34m(json_path)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_character_file\u001b[39m(json_path: \u001b[38;5;28mstr\u001b[39m) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m json.load(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dimas\\NLP_w2\\ProjectTemplate\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:344\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    339\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './notebooks/Character Library/Crime_punishment.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Span\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment import vader\n",
    "from collections import defaultdict\n",
    "\n",
    "# --------- Config (uses your existing CONFIG and load/normalize) ---------\n",
    "CONFIG = {\n",
    "    \"book1_path\": \"../data/Crime-punishment.txt\",\n",
    "    \"book2_path\": \"../data/The-Brotherskaramazov.txt\",\n",
    "    \"language\": \"en\",\n",
    "    \"use_stopwords\": False,\n",
    "    \"min_ngram_count\": 5,\n",
    "    \"top_k\": 20\n",
    "}\n",
    "\n",
    "# If you already have these functions defined, comment these stubs out.\n",
    "def load_text(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    # Basic normalization: collapse whitespace, normalize quotes; keep punctuation for NLP.\n",
    "    text = text.replace('\\u201c', '\"').replace('\\u201d', '\"').replace('\\u2019', \"'\")\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# --------- Character metadata loaders ---------\n",
    "def load_character_file(json_path: str) -> Dict[str, Any]:\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def build_aliases(char_dict: Dict[str, Any]) -> Dict[str, List[str]]:\n",
    "    # Expected format:\n",
    "    # {\n",
    "    #   \"main_char\": \"Rodion_Raskolnikov\",\n",
    "    #   \"aliases\": [\"Raskolnikov\", \"Rodion\", \"Rodya\", \"Romanovitch\", ...]\n",
    "    # }\n",
    "    # For Brothers:\n",
    "    # {\n",
    "    #   \"main_chars\": [\"Dmitri_Karamazov\", \"Ivan_Karamazov\", \"Alexei_Karamazov\"],\n",
    "    #   \"aliases\": {\n",
    "    #       \"Dmitri_Karamazov\": [\"Mitya\", \"Dmitri\", \"Karamazov (Dmitri context)\"],\n",
    "    #       ...\n",
    "    #   }\n",
    "    # }\n",
    "    aliases_map = {}\n",
    "    if \"main_char\" in char_dict:\n",
    "        mc = char_dict[\"main_char\"]\n",
    "        aliases_map[mc] = [mc] + char_dict.get(\"aliases\", [])\n",
    "    elif \"main_chars\" in char_dict and \"aliases\" in char_dict:\n",
    "        for mc in char_dict[\"main_chars\"]:\n",
    "            aliases_map[mc] = [mc] + char_dict[\"aliases\"].get(mc, [])\n",
    "    else:\n",
    "        # Fallback: if JSON is simply { \"Rodion_Raskolnikov\": [\"Raskolnikov\", ...], ... }\n",
    "        for mc, als in char_dict.items():\n",
    "            if isinstance(als, list):\n",
    "                aliases_map[mc] = [mc] + als\n",
    "    # Normalize alias strings (lowercase stripped)\n",
    "    for k in aliases_map:\n",
    "        aliases_map[k] = sorted(set([a.strip() for a in aliases_map[k] if a.strip()]))\n",
    "    return aliases_map\n",
    "\n",
    "# --------- NLP initialization ---------\n",
    "def init_nlp():\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    # Add sentence boundary if needed (spaCy has sentencizer in pipeline)\n",
    "    return nlp\n",
    "\n",
    "def init_sentiment():\n",
    "    try:\n",
    "        sid = vader.SentimentIntensityAnalyzer()\n",
    "    except LookupError:\n",
    "        nltk.download('vader_lexicon')\n",
    "        sid = vader.SentimentIntensityAnalyzer()\n",
    "    return sid\n",
    "\n",
    "# --------- Decision lexicons and weights ---------\n",
    "DECISION_VERBS_NEG = {\n",
    "    \"kill\": -1.0, \"murder\": -1.0, \"slay\": -0.9, \"assassinate\": -1.0,\n",
    "    \"rob\": -0.8, \"steal\": -0.8, \"theft\": -0.8,\n",
    "    \"betray\": -0.9, \"deceive\": -0.7, \"lie\": -0.6,\n",
    "    \"confess to murder\": -0.6, \"commit crime\": -0.9,\n",
    "    \"attempt murder\": -0.9, \"strike\": -0.5, \"beat\": -0.7, \"violence\": -0.9\n",
    "}\n",
    "\n",
    "DECISION_VERBS_POS = {\n",
    "    \"help\": +0.7, \"save\": +0.9, \"protect\": +0.8, \"forgive\": +0.8,\n",
    "    \"repent\": +0.6, \"confess\": +0.4, \"donate\": +0.6, \"pray\": +0.4,\n",
    "    \"reconcile\": +0.5, \"apologize\": +0.5, \"care\": +0.6, \"comfort\": +0.6\n",
    "}\n",
    "\n",
    "# Multi-word phrases mapped to polarity\n",
    "DECISION_PHRASES = {\n",
    "    \"commit murder\": -1.0, \"plan murder\": -0.9, \"confess murder\": -0.6,\n",
    "    \"go to police\": +0.3, \"turn himself in\": +0.5,\n",
    "    \"help her\": +0.6, \"help him\": +0.6, \"help them\": +0.6\n",
    "}\n",
    "\n",
    "# Neutral/administrative triggers (optionally ignored or low weight)\n",
    "DECISION_VERBS_NEU = {\n",
    "    \"decide\": 0.0, \"choose\": 0.0, \"think\": 0.0, \"consider\": 0.0\n",
    "}\n",
    "\n",
    "# --------- Utility: character match ---------\n",
    "def matches_character(span_text: str, aliases: List[str]) -> bool:\n",
    "    st = span_text.lower()\n",
    "    for al in aliases:\n",
    "        if al.lower() in st:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def sentence_mentions_character(sent: Span, aliases: List[str]) -> bool:\n",
    "    # Check NER PERSON entities and raw text for alias substrings\n",
    "    text_l = sent.text.lower()\n",
    "    for al in aliases:\n",
    "        if al.lower() in text_l:\n",
    "            return True\n",
    "    # NER check\n",
    "    for ent in sent.ents:\n",
    "        if ent.label_ == \"PERSON\" and ent.text.lower() in [a.lower() for a in aliases]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# --------- Decision detection ---------\n",
    "def detect_decisions_in_sentence(sent: Span) -> List[Dict[str, Any]]:\n",
    "    # Extract lemmatized verbs and detect phrases\n",
    "    tokens = [t for t in sent if not t.is_space]\n",
    "    lemmas = [t.lemma_.lower() for t in tokens]\n",
    "\n",
    "    found = []\n",
    "\n",
    "    # Phrase detection\n",
    "    text_l = sent.text.lower()\n",
    "    for phrase, w in DECISION_PHRASES.items():\n",
    "        if phrase in text_l:\n",
    "            found.append({\"type\": \"phrase\", \"key\": phrase, \"weight\": w})\n",
    "\n",
    "    # Verb detection\n",
    "    for t in tokens:\n",
    "        if t.pos_ == \"VERB\":\n",
    "            lemma = t.lemma_.lower()\n",
    "            if lemma in DECISION_VERBS_NEG:\n",
    "                found.append({\"type\": \"verb\", \"key\": lemma, \"weight\": DECISION_VERBS_NEG[lemma], \"token\": t})\n",
    "            elif lemma in DECISION_VERBS_POS:\n",
    "                found.append({\"type\": \"verb\", \"key\": lemma, \"weight\": DECISION_VERBS_POS[lemma], \"token\": t})\n",
    "            elif lemma in DECISION_VERBS_NEU:\n",
    "                # Usually skip neutral unless combined with crime nouns\n",
    "                pass\n",
    "\n",
    "    # Crime nouns (supporting evidence)\n",
    "    crime_nouns = {\"murder\", \"crime\", \"blood\", \"axe\", \"weapon\", \"theft\", \"robbery\"}\n",
    "    nouns = set([t.lemma_.lower() for t in tokens if t.pos_ in (\"NOUN\", \"PROPN\")])\n",
    "    has_crime_context = len(crime_nouns.intersection(nouns)) > 0\n",
    "\n",
    "    # If only neutral verbs + crime context, treat as negative decision\n",
    "    if has_crime_context and not found:\n",
    "        found.append({\"type\": \"context\", \"key\": \"crime_context\", \"weight\": -0.4})\n",
    "\n",
    "    return found\n",
    "\n",
    "# --------- Link decision to subject (character agency) ---------\n",
    "def character_is_agent_of_decision(sent: Span, char_aliases: List[str]) -> bool:\n",
    "    # Try dependency: look for subject of the decision verb matching character mention\n",
    "    # Fallback: if character is mentioned and first-person pronouns could link (not perfect).\n",
    "    # Simple heuristic: character must be mentioned in sentence.\n",
    "    if not sentence_mentions_character(sent, char_aliases):\n",
    "        return False\n",
    "\n",
    "    # Dependency heuristic\n",
    "    # If a verb is present, check nsubj text overlap with aliases\n",
    "    for t in sent:\n",
    "        if t.pos_ == \"VERB\":\n",
    "            for child in t.children:\n",
    "                if child.dep_ in (\"nsubj\", \"nsubjpass\"):\n",
    "                    subj_text = child.text.lower()\n",
    "                    # Check alias exact match or substring\n",
    "                    for al in char_aliases:\n",
    "                        if al.lower() in subj_text:\n",
    "                            return True\n",
    "\n",
    "    # If not found, we still allow if character is mentioned and verbs present (looser heuristic)\n",
    "    return True\n",
    "\n",
    "# --------- Sentiment scoring ---------\n",
    "def sentiment_impact_score(sent: Span, sid: vader.SentimentIntensityAnalyzer, base_weight: float) -> float:\n",
    "    # Use VADER sentiment around sentence. Combine base decision polarity with sentiment polarity.\n",
    "    s = sid.polarity_scores(sent.text)\n",
    "    compound = s[\"compound\"]  # [-1, 1]\n",
    "    # Impact: base_weight + compound*|base_weight| (amplify/reduce based on sentiment)\n",
    "    impact = base_weight + compound * abs(base_weight) * 0.6\n",
    "    return impact\n",
    "\n",
    "# --------- Processing pipeline ---------\n",
    "def process_book(text: str, nlp, sid, aliases_map: Dict[str, List[str]]) -> Dict[str, List[Tuple[int, float, Dict[str, Any], str]]]:\n",
    "    \"\"\"\n",
    "    Returns: dict character -> list of (sentence_index, impact_score, decision_meta, sentence_text)\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    results = defaultdict(list)\n",
    "    for i, sent in enumerate(doc.sents):\n",
    "        decisions = detect_decisions_in_sentence(sent)\n",
    "        if not decisions:\n",
    "            continue\n",
    "        # For each character, check agency in this sentence\n",
    "        for char, aliases in aliases_map.items():\n",
    "            if character_is_agent_of_decision(sent, aliases):\n",
    "                for d in decisions:\n",
    "                    impact = sentiment_impact_score(sent, sid, d[\"weight\"])\n",
    "                    results[char].append((i, impact, d, sent.text))\n",
    "    return results\n",
    "\n",
    "# --------- Aggregate and plot ---------\n",
    "def build_time_series(decisions: List[Tuple[int, float, Dict[str, Any], str]]) -> Tuple[List[int], List[float], List[Dict[str, Any]]]:\n",
    "    # Sort by sentence index and compute cumulative impact\n",
    "    decisions_sorted = sorted(decisions, key=lambda x: x[0])\n",
    "    times = []\n",
    "    cum = []\n",
    "    meta_list = []\n",
    "    total = 0.0\n",
    "    for idx, impact, meta, sent_text in decisions_sorted:\n",
    "        total += impact\n",
    "        times.append(idx)\n",
    "        cum.append(total)\n",
    "        meta_list.append({\"impact\": impact, \"meta\": meta, \"sentence\": sent_text, \"index\": idx})\n",
    "    return times, cum, meta_list\n",
    "\n",
    "def plot_character_series(char: str, times: List[int], series: List[float], meta_list: List[Dict[str, Any]], out_dir: str):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(times, series, marker='o', linewidth=2, label=char)\n",
    "    plt.title(f\"Decision Impact Over Narrative Time: {char}\")\n",
    "    plt.xlabel(\"Sentence index (narrative time)\")\n",
    "    plt.ylabel(\"Cumulative impact\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    # Annotate key points (top negative and top positive)\n",
    "    if meta_list:\n",
    "        # pick top 3 by |impact|\n",
    "        top_points = sorted(meta_list, key=lambda m: abs(m[\"impact\"]), reverse=True)[:3]\n",
    "        for tp in top_points:\n",
    "            plt.scatter(tp[\"index\"], series[times.index(tp[\"index\"])], color='red' if tp[\"impact\"] < 0 else 'green')\n",
    "            label = f'{tp[\"meta\"][\"key\"]} ({tp[\"impact\"]:+.2f})'\n",
    "            plt.annotate(label, (tp[\"index\"], series[times.index(tp[\"index\"])]),\n",
    "                         textcoords=\"offset points\", xytext=(5,5), fontsize=8)\n",
    "    plt.legend()\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_path = os.path.join(out_dir, f\"{char}_impact.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# --------- Main runner ---------\n",
    "def main():\n",
    "    # Init NLP tools\n",
    "    nlp = init_nlp()\n",
    "    sid = init_sentiment()\n",
    "\n",
    "    # Load and normalize texts (from your setup)\n",
    "    text1 = normalize_text(load_text(CONFIG[\"book1_path\"]))\n",
    "    text2 = normalize_text(load_text(CONFIG[\"book2_path\"]))\n",
    "\n",
    "    # Load character JSONs\n",
    "    crime_char_json = load_character_file(\"./notebooks/Character Library/Crime_punishment.json\")\n",
    "    bros_char_json = load_character_file(\"./notebooks/Character Library/The_brothers.json\")\n",
    "\n",
    "    crime_aliases = build_aliases(crime_char_json)   # expects Rodion_Raskolnikov aliases\n",
    "    bros_aliases = build_aliases(bros_char_json)     # expects Dmitri, Ivan, Alexei maps\n",
    "\n",
    "    # Process books\n",
    "    print(\"Processing Crime and Punishment...\")\n",
    "    decisions_crime = process_book(text1, nlp, sid, crime_aliases)\n",
    "\n",
    "    print(\"Processing The Brothers Karamazov...\")\n",
    "    decisions_bros = process_book(text2, nlp, sid, bros_aliases)\n",
    "\n",
    "    # Build and plot\n",
    "    out_dir = \"./outputs\"\n",
    "    for char, decs in decisions_crime.items():\n",
    "        times, series, meta_list = build_time_series(decs)\n",
    "        plot_character_series(char, times, series, meta_list, out_dir)\n",
    "        print(f\"{char}: {len(decs)} decisions detected. Plot saved to {out_dir}/{char}_impact.png\")\n",
    "\n",
    "    for char, decs in decisions_bros.items():\n",
    "        times, series, meta_list = build_time_series(decs)\n",
    "        plot_character_series(char, times, series, meta_list, out_dir)\n",
    "        print(f\"{char}: {len(decs)} decisions detected. Plot saved to {out_dir}/{char}_impact.png\")\n",
    "\n",
    "    # Optional: export raw decision logs for inspection\n",
    "    def export_logs(book_name: str, decisions_map: Dict[str, List[Tuple[int, float, Dict[str, Any], str]]]):\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        out_path = os.path.join(out_dir, f\"{book_name}_decision_log.json\")\n",
    "        serializable = {}\n",
    "        for char, decs in decisions_map.items():\n",
    "            serializable[char] = [\n",
    "                {\"sentence_index\": idx, \"impact\": impact, \"meta\": meta, \"sentence\": sent}\n",
    "                for idx, impact, meta, sent in decs\n",
    "            ]\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(serializable, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Exported decision log: {out_path}\")\n",
    "\n",
    "    export_logs(\"crime_and_punishment\", decisions_crime)\n",
    "    export_logs(\"brothers_karamazov\", decisions_bros)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793aabb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568005,\n",
       " ['chapter',\n",
       "  'i',\n",
       "  'chapter',\n",
       "  'ii',\n",
       "  'chapter',\n",
       "  'iii',\n",
       "  'chapter',\n",
       "  'iv',\n",
       "  'chapter',\n",
       "  'v',\n",
       "  'chapter',\n",
       "  'vi'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_text(p: str) -> str:\n",
    "    with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    # 1) strip Gutenberg header/footer FIRST\n",
    "    t = strip_gutenberg(t)\n",
    "    # 2) join hyphenated line breaks (e.g., \"won-\\nderful\")\n",
    "    t = re.sub(r\"-\\s*\\n\", \"\", t)\n",
    "    # 3) normalize whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n",
    "\n",
    "text1 = normalize_text(load_text(CONFIG[\"book1_path\"]))\n",
    "text2 = normalize_text(load_text(CONFIG[\"book2_path\"]))\n",
    "\n",
    "tokens1 = WORD_RE.findall(text1.lower())\n",
    "tokens2 = WORD_RE.findall(text2.lower())\n",
    "\n",
    "if CONFIG[\"use_stopwords\"]:\n",
    "    tokens1 = [t for t in tokens1 if t not in STOPWORDS]\n",
    "    tokens2 = [t for t in tokens2 if t not in STOPWORDS]\n",
    "\n",
    "tokens = tokens1 + tokens2\n",
    "\n",
    "len(tokens1), len(tokens2), len(tokens)\n",
    "\n",
    "len(tokens), tokens[:12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a22e04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568005,\n",
       " ['chapter',\n",
       "  'i',\n",
       "  'chapter',\n",
       "  'ii',\n",
       "  'chapter',\n",
       "  'iii',\n",
       "  'chapter',\n",
       "  'iv',\n",
       "  'chapter',\n",
       "  'v',\n",
       "  'chapter',\n",
       "  'vi'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_text(p: str) -> str:\n",
    "    with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    # 1) strip Gutenberg header/footer FIRST\n",
    "    t = strip_gutenberg(t)\n",
    "    # 2) join hyphenated line breaks (e.g., \"won-\\nderful\")\n",
    "    t = re.sub(r\"-\\s*\\n\", \"\", t)\n",
    "    # 3) normalize whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n",
    "\n",
    "text1 = normalize_text(load_text(CONFIG[\"book1_path\"]))\n",
    "text2 = normalize_text(load_text(CONFIG[\"book2_path\"]))\n",
    "\n",
    "tokens1 = WORD_RE.findall(text1.lower())\n",
    "tokens2 = WORD_RE.findall(text2.lower())\n",
    "\n",
    "if CONFIG[\"use_stopwords\"]:\n",
    "    tokens1 = [t for t in tokens1 if t not in STOPWORDS]\n",
    "    tokens2 = [t for t in tokens2 if t not in STOPWORDS]\n",
    "\n",
    "tokens = tokens1 + tokens2\n",
    "\n",
    "len(tokens1), len(tokens2), len(tokens)\n",
    "\n",
    "len(tokens), tokens[:12]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
