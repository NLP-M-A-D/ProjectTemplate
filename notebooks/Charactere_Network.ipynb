{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a65cb4e7",
   "metadata": {},
   "source": [
    "# ðŸ“˜Measuring Raskolnikovâ€™s Social Connections\n",
    "\n",
    "In this notebook, I look at how Raskolnikovâ€™s psychological state shifts across *Crime and Punishment* by tracking his social interactions. The idea is simple: when characters appear close to each other in the text, it usually means they are involved in the same moment or scene. So I use **character co-occurrence** as a way to measure how socially â€œpresentâ€ he is.\n",
    "\n",
    "I compare the **first ten chapters** with the **last ten chapters** to see if he becomes more connected (and less isolated) as the story moves forward. This helps show whether his mental state is drifting deeper into isolation or slowly reconnecting with people around him. ðŸ“Šâœ¨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af0084a",
   "metadata": {},
   "source": [
    "### Method\n",
    "\n",
    "To study how Raskolnikov interacts with others, I split the book into two phases:  \n",
    "**the first 10 chapters** (early) and **the last 10 chapters** (late). This gives a clear before/after view of his social behavior.\n",
    "\n",
    "Using a characterâ€“alias dictionary (so names like *Rodya*, *Rodion*, and *Raskolnikov* all map to the same person), I scanned the text with a sliding window of **Â±40 tokens**. If two character names appeared in that window, I counted them as interacting.\n",
    "\n",
    "For each phase, I measured two things:\n",
    "\n",
    "- **Degree** â€“ how many different characters Raskolnikov appears with  \n",
    "- **Total Weighted Co-occurrence** â€“ how often he appears with them (strength of connection)\n",
    "\n",
    "These numbers were then used to build both static (NetworkX) and interactive (PyVis) social network graphs. The goal is to see how his â€œsocial worldâ€ expands or contracts across the story. ðŸ”ðŸ“Œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65431f26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T17:01:13.982939Z",
     "start_time": "2025-11-13T17:01:12.368326Z"
    }
   },
   "outputs": [],
   "source": [
    "import re, os, math, json, collections\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set default figure size + turn on grid for all plots\n",
    "plt.rcParams[\"figure.figsize\"] = (9, 4.5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "# main configuration for the character network notebook\n",
    "CONFIG = {\n",
    "    \"book1_path\": \"../data/Crime-punishment.txt\",   # path to the Crime & Punishment text\n",
    "    \"language\": \"en\",\n",
    "    \"use_stopwords\": True,                          # whether we remove stopwords or not\n",
    "    \"min_ngram_count\": 5,                           # ignore rare n-grams\n",
    "    \"top_k\": 20,                                    # how many strongest pairs to show\n",
    "    \"char_json\": \"./Character Library/Crime_punishment.json\"  # json with character aliases\n",
    "}\n",
    "\n",
    "# regex to match clean word tokens (handles apostrophes and hyphens too)\n",
    "WORD_RE = re.compile(r\"[^\\W\\d_]+(?:['-][^\\W\\d_]+)*\", flags=re.UNICODE)\n",
    "\n",
    "# simple English stopword list â€” helps reduce noise in n-grams\n",
    "STOPWORDS = {\n",
    "    \"the\",\"a\",\"an\",\"and\",\"or\",\"but\",\"if\",\"so\",\"because\",\"as\",\n",
    "    \"of\",\"in\",\"on\",\"at\",\"to\",\"for\",\"from\",\"by\",\"with\",\"about\",\n",
    "    \"this\",\"that\",\"these\",\"those\",\n",
    "    \"i\",\"you\",\"he\",\"she\",\"it\",\"we\",\"they\",\"me\",\"him\",\"her\",\"them\",\n",
    "    \"my\",\"your\",\"his\",\"her\",\"its\",\"our\",\"their\",\n",
    "    \"is\",\"am\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\n",
    "    \"do\",\"does\",\"did\",\"have\",\"has\",\"had\",\n",
    "    \"not\",\"no\",\"yes\",\"there\",\"here\",\"then\",\"than\",\n",
    "    \"what\",\"which\",\"who\",\"whom\",\"whose\",\"when\",\"where\",\"why\",\"how\",\n",
    "    \"up\",\"down\",\"out\",\"into\",\"over\",\"under\",\"again\",\"further\",\n",
    "    \"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"most\",\"other\",\"some\",\"such\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11547f59",
   "metadata": {},
   "source": [
    "## 1. Load & Normalize Text\n",
    "\n",
    "Before anything else, the raw book needs to be cleaned so the analysis is stable. I fix things like:\n",
    "\n",
    "- hyphenated line breaks (words split across lines)\n",
    "- messy or inconsistent whitespace\n",
    "- lowercase everything for easier matching\n",
    "\n",
    "Since the text comes from Project Gutenberg, it also includes extra boilerplate that needs to be removed first. This keeps the book clean and ready for the character network analysis. âœ¨ðŸ“˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e0187e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T20:22:59.852224Z",
     "start_time": "2025-11-11T20:22:59.844505Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Boilerplate/front-matter stripper tuned for Dostoevsky releases ---------\n",
    "# These markers help us remove the Gutenberg header/footer correctly.\n",
    "\n",
    "_GB_START_MARKERS = [\n",
    "    r\"\\*\\*\\*\\s*START OF (THIS|THE) PROJECT GUTENBERG EBOOK\",   # modern\n",
    "    r\"START OF (THIS|THE) PROJECT GUTENBERG EBOOK\",             # fallback\n",
    "    r\"End of the Project Gutenberg(?:'s)? Etext\",               # very old variants\n",
    "]\n",
    "\n",
    "_GB_END_MARKERS = [\n",
    "    r\"\\*\\*\\*\\s*END OF (THIS|THE) PROJECT GUTENBERG EBOOK\",      # modern\n",
    "    r\"END OF (THIS|THE) PROJECT GUTENBERG EBOOK\",                # fallback\n",
    "    r\"End of Project Gutenberg(?:'s)? (?:Etext|eBook)\",          # older variants\n",
    "    r\"\\*\\*\\*\\s*END: FULL LICENSE\\s*\\*\\*\\*\",                      # license block end (older)\n",
    "]\n",
    "\n",
    "# stronger hints specific to the Dostoevsky translations (useful when no GB markers exist)\n",
    "_START_HINTS = [\n",
    "    r\"^\\s*PART\\s+[IVXLCDM0-9]+[\\.\\:\\-\\s]\",    # PART I / PART 1\n",
    "    r\"^\\s*BOOK\\s+[IVXLCDM0-9]+[\\.\\:\\-\\s]\",    # BOOK I / BOOK ONE\n",
    "    r\"^\\s*CHAPTER\\s+[IVXLCDM0-9]+[\\.\\:\\-\\s]\", # CHAPTER I / CHAPTER 1\n",
    "]\n",
    "\n",
    "# hints for trimming the end of the novel\n",
    "_END_HINTS = [\n",
    "    r\"^\\s*EPILOGUE\\b.*\",                      # keep epilogue, but stop after any END marker\n",
    "    r\"^\\s*THE\\s+END\\s*$\",\n",
    "    r\"^\\s*END OF (?:PART|BOOK|VOLUME)\\s+[IVXLCDM0-9]+\\s*$\",\n",
    "]\n",
    "\n",
    "def _trim_to_first_hint(text: str, hints) -> str:\n",
    "    # find the first place where the real book starts\n",
    "    for pat in hints:\n",
    "        m = re.search(pat, text, flags=re.IGNORECASE | re.MULTILINE)\n",
    "        if m:\n",
    "            return text[m.start():]\n",
    "    return text\n",
    "\n",
    "def _trim_after_last_hint(text: str, hints) -> str:\n",
    "    # find the last structural hint (like \"THE END\")\n",
    "    end_pos = None\n",
    "    for pat in hints:\n",
    "        for m in re.finditer(pat, text, flags=re.IGNORECASE | re.MULTILINE):\n",
    "            end_pos = max(end_pos or 0, m.start())\n",
    "    return text[:end_pos] if end_pos else text\n",
    "\n",
    "def strip_gutenberg(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the main book body:\n",
    "      1) Prefer explicit Gutenberg START/END markers.\n",
    "      2) If missing, fall back to Dostoevsky-friendly chapter/part/book hints.\n",
    "      3) Trim common tails like THE END / license blocks.\n",
    "    \"\"\"\n",
    "    # remove BOM if it exists\n",
    "    t = text.replace(\"\\ufeff\", \"\")\n",
    "\n",
    "    # try to find the start using the strong Gutenberg markers\n",
    "    start_idx = None\n",
    "    for pat in _GB_START_MARKERS:\n",
    "        m = re.search(pat, t, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            start_idx = t.find(\"\\n\", m.end())\n",
    "            if start_idx == -1:\n",
    "                start_idx = m.end()\n",
    "            break\n",
    "\n",
    "    # same idea for the end\n",
    "    end_idx = None\n",
    "    for pat in _GB_END_MARKERS:\n",
    "        m = re.search(pat, t, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            end_idx = m.start()\n",
    "            break\n",
    "\n",
    "    # if we found both markers, we trust them fully\n",
    "    if start_idx is not None and end_idx is not None and end_idx > start_idx:\n",
    "        core = t[start_idx:end_idx]\n",
    "    else:\n",
    "        # fallback: use chapter / part / book hints to find approximate start\n",
    "        core = _trim_to_first_hint(t, _START_HINTS)\n",
    "\n",
    "        # if still nothing matched, try super generic hints\n",
    "        if core is t:\n",
    "            generic_hints = [\n",
    "                r\"^\\s*chapter\\s+[ivxlcdm0-9]+[\\.\\:\\-\\s]\",\n",
    "                r\"^\\s*book\\s+[ivxlcdm0-9]+[\\.\\:\\-\\s]\",\n",
    "                r\"^\\s*part\\s+[ivxlcdm0-9]+[\\.\\:\\-\\s]\",\n",
    "            ]\n",
    "            core = _trim_to_first_hint(t, generic_hints)\n",
    "\n",
    "        # trim the end using known end hints\n",
    "        core = _trim_after_last_hint(core, _END_HINTS)\n",
    "\n",
    "        # if Gutenberg end marker still sneaks inside, remove that too\n",
    "        for pat in _GB_END_MARKERS:\n",
    "            m = re.search(pat, core, flags=re.IGNORECASE)\n",
    "            if m:\n",
    "                core = core[:m.start()]\n",
    "                break\n",
    "\n",
    "    # remove leftover license / Project Gutenberg text blocks\n",
    "    core = re.sub(r\"\\n\\s*End of the Project Gutenberg.*\", \"\", core, flags=re.IGNORECASE)\n",
    "    core = re.sub(r\"\\*\\*\\*\\s*START: FULL LICENSE\\s*\\*\\*\\*.*\", \"\", core,\n",
    "                  flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    # remove URLs, trailing spaces, and collapse big blank areas\n",
    "    core = re.sub(r\"https?://\\S+\", \"\", core)\n",
    "    core = re.sub(r\"[ \\t]+\\n\", \"\\n\", core)\n",
    "    core = re.sub(r\"\\n{3,}\", \"\\n\\n\", core)\n",
    "\n",
    "    return core.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42845e25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T20:23:04.383862Z",
     "start_time": "2025-11-11T20:23:03.718342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total detected chapters: 41\n",
      "All tokens: 101759 | Early tokens: 25463 | Late tokens: 20950\n",
      "Early sample: ['exceptionally', 'hot', 'evening', 'early', 'july', 'young', 'man', 'came', 'garret', 'lodged', 's', 'place']\n",
      "Late sample: ['strange', 'period', 'began', 'raskolnikov', 'though', 'fog', 'fallen', 'upon', 'wrapped', 'dreary', 'solitude', 'escape']\n"
     ]
    }
   ],
   "source": [
    "# --- helper so it works whether your notebook is in project root or in notebooks/ ---\n",
    "def _path_in_data(fname: str) -> str:\n",
    "    # try to find the file inside ./data or ../data\n",
    "    for p in (Path(\"./data\")/fname, Path(\"../data\")/fname):\n",
    "        if p.exists():\n",
    "            return str(p)\n",
    "    # if nothing found â†’ throw error\n",
    "    raise FileNotFoundError(f\"Couldn't find {fname} in ./data or ../data\")\n",
    "\n",
    "# Point to dataset (only Crime and Punishment)\n",
    "CONFIG[\"book1_path\"] = _path_in_data(\"Crime-punishment.txt\")\n",
    "\n",
    "def load_text(p: str) -> str:\n",
    "    # load raw text from file\n",
    "    with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    # 1) remove all Gutenberg junk at the top/bottom\n",
    "    t = strip_gutenberg(t)\n",
    "    # 1.5) normalize curly quotes so contractions stay correct\n",
    "    t = t.replace(\"â€™\", \"'\").replace(\"â€˜\", \"'\")\n",
    "    # 2) fix hyphenated line-break words (e.g., \"won-\\nderful\")\n",
    "    t = re.sub(r\"-\\s*\\n\", \"\", t)\n",
    "    # 3) collapse weird whitespace and clean things up\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t.strip()\n",
    "\n",
    "# Load + normalize the book\n",
    "text1 = normalize_text(load_text(CONFIG[\"book1_path\"]))   # Crime and Punishment\n",
    "\n",
    "# --- Split into chapters based on headings like \"CHAPTER I\", \"CHAPTER II\", ... ---\n",
    "chapters_raw = re.split(r\"\\bchapter\\s+[ivxlcdm0-9]+\\b\", text1, flags=re.IGNORECASE)\n",
    "# clean empty pieces from the split\n",
    "chapters = [ch.strip() for ch in chapters_raw if ch.strip()]\n",
    "\n",
    "print(\"Total detected chapters:\", len(chapters))\n",
    "\n",
    "# --- Define Early (first 10) and Late (last 10) ---\n",
    "early_chapters = chapters[:10]\n",
    "late_chapters  = chapters[-10:]\n",
    "\n",
    "# turn chapter pieces into long text strings\n",
    "early_text = \" \".join(early_chapters)\n",
    "late_text  = \" \".join(late_chapters)\n",
    "\n",
    "# --- Tokenize all three versions ---\n",
    "def tokenize(text):\n",
    "    # find all valid words using regex\n",
    "    toks = WORD_RE.findall(text.lower())\n",
    "    # optionally remove stopwords\n",
    "    if CONFIG.get(\"use_stopwords\", False):\n",
    "        toks = [t for t in toks if t not in STOPWORDS]\n",
    "    return toks\n",
    "\n",
    "# tokenizing everything\n",
    "tokens_all   = tokenize(text1)\n",
    "tokens_early = tokenize(early_text)\n",
    "tokens_late  = tokenize(late_text)\n",
    "\n",
    "# basic checks to see how much text we have in each segment\n",
    "print(\"All tokens:\", len(tokens_all),\n",
    "      \"| Early tokens:\", len(tokens_early),\n",
    "      \"| Late tokens:\", len(tokens_late))\n",
    "\n",
    "print(\"Early sample:\", tokens_early[:12])\n",
    "print(\"Late sample:\", tokens_late[:12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "439e5429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters in library: ['Rodion_Raskolnikov', 'Sonia_Marmeladov', 'Avdotya_Raskolnikov', 'Arkady_Svidrigailov', 'Dmitri_Razumikhin', 'Pyotr_Luzhin', 'Porfiry_Petrovich', 'Semyon_Marmeladov', 'Katerina_Ivanovna', 'Alyona_Ivanovna', 'Lizaveta_Ivanovna', 'Pulcheria_Alexandrovna', 'Zossimov', 'Alexander_Zametov', 'Andrey_Lebezyatnikov']\n",
      "Total aliases: 96\n"
     ]
    }
   ],
   "source": [
    "with open(CONFIG[\"char_json\"], \"r\", encoding=\"utf-8\") as f:\n",
    "    char_lib = json.load(f)\n",
    "\n",
    "# quick check: list all characters found in the json file\n",
    "print(\"Characters in library:\", list(char_lib.keys()))\n",
    "\n",
    "alias2char = {}\n",
    "\n",
    "# build a dictionary that maps each alias â†’ main character name\n",
    "for char_name, info in char_lib.items():\n",
    "    aliases = info.get(\"aliases\", [])\n",
    "\n",
    "    for a in aliases:\n",
    "        # some aliases are just strings\n",
    "        if isinstance(a, str):\n",
    "            alias = a\n",
    "        # some are dicts with {\"text\": ..., \"tier\": ...}\n",
    "        elif isinstance(a, dict) and \"text\" in a:\n",
    "            alias = a[\"text\"]\n",
    "        else:\n",
    "            continue  # ignore anything else\n",
    "\n",
    "        # store lowercase version for easier matching later\n",
    "        alias2char[alias.lower()] = char_name\n",
    "\n",
    "print(\"Total aliases:\", len(alias2char))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "456e0fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def character_cooccurrence(tokens, alias2char, window=40, step=5):\n",
    "    \"\"\"\n",
    "    Co-occurrence with *partially overlapping* windows.\n",
    "\n",
    "    tokens     : list of tokens\n",
    "    alias2char : alias -> canonical character name\n",
    "    window     : number of tokens in each window\n",
    "    step       : how far we move the window each time\n",
    "                 - step=1  -> very dense, huge weights (what gave 1216 / 2957)\n",
    "                 - step=window -> non-overlapping, small weights\n",
    "                 - step=5,10,... -> in between\n",
    "    \"\"\"\n",
    "\n",
    "    pairs = Counter()\n",
    "    n = len(tokens)\n",
    "\n",
    "    for start in range(0, n - window + 1, step):\n",
    "        window_tokens = tokens[start:start + window]\n",
    "\n",
    "        # which characters appear in this window?\n",
    "        chars_in_window = set()\n",
    "        for tok in window_tokens:\n",
    "            name = alias2char.get(tok.lower())\n",
    "            if name:\n",
    "                chars_in_window.add(name)\n",
    "\n",
    "        chars_in_window = list(chars_in_window)\n",
    "        for i in range(len(chars_in_window)):\n",
    "            for j in range(i + 1, len(chars_in_window)):\n",
    "                pair = tuple(sorted((chars_in_window[i], chars_in_window[j])))\n",
    "                pairs[pair] += 1\n",
    "\n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716592be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early pairs: 17\n",
      "Late pairs : 30\n"
     ]
    }
   ],
   "source": [
    "# Early and late character co-occurrence\n",
    "# (we check which characters appear close to each other inside a window of 40 words)\n",
    "\n",
    "early_pairs = character_cooccurrence(tokens_early, alias2char, window=40, step=5)\n",
    "late_pairs  = character_cooccurrence(tokens_late,  alias2char, window=40, step=5)\n",
    "\n",
    "# basic check: how many characterâ€“character links we detected\n",
    "print(\"Early pairs:\", len(early_pairs))\n",
    "print(\"Late pairs :\", len(late_pairs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "63540a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Raskolnikov connections (top 20):\n",
      "Rodion_Raskolnikov â€“ Avdotya_Raskolnikov: 69\n",
      "Rodion_Raskolnikov â€“ Semyon_Marmeladov: 60\n",
      "Rodion_Raskolnikov â€“ Lizaveta_Ivanovna: 36\n",
      "Rodion_Raskolnikov â€“ Katerina_Ivanovna: 22\n",
      "Rodion_Raskolnikov â€“ Sonia_Marmeladov: 21\n",
      "Rodion_Raskolnikov â€“ Zossimov: 15\n",
      "Rodion_Raskolnikov â€“ Alexander_Zametov: 9\n",
      "Rodion_Raskolnikov â€“ Pulcheria_Alexandrovna: 8\n",
      "Rodion_Raskolnikov â€“ Pyotr_Luzhin: 3\n",
      "\n",
      "Late Raskolnikov connections (top 20):\n",
      "Rodion_Raskolnikov â€“ Sonia_Marmeladov: 156\n",
      "Rodion_Raskolnikov â€“ Porfiry_Petrovich: 156\n",
      "Rodion_Raskolnikov â€“ Avdotya_Raskolnikov: 137\n",
      "Rodion_Raskolnikov â€“ Pulcheria_Alexandrovna: 60\n",
      "Rodion_Raskolnikov â€“ Alexander_Zametov: 22\n",
      "Rodion_Raskolnikov â€“ Dmitri_Razumikhin: 17\n",
      "Rodion_Raskolnikov â€“ Katerina_Ivanovna: 14\n",
      "Rodion_Raskolnikov â€“ Lizaveta_Ivanovna: 8\n",
      "Rodion_Raskolnikov â€“ Semyon_Marmeladov: 7\n",
      "Rodion_Raskolnikov â€“ Zossimov: 6\n",
      "Rodion_Raskolnikov â€“ Pyotr_Luzhin: 2\n",
      "Rodion_Raskolnikov â€“ Arkady_Svidrigailov: 2\n"
     ]
    }
   ],
   "source": [
    "MAIN_CHAR = \"Rodion_Raskolnikov\"\n",
    "\n",
    "def connections_for(char_name, pair_counts, top_n=20):\n",
    "    # find all pairs where our target character appears\n",
    "    edges = []\n",
    "    for (c1, c2), cnt in pair_counts.items():\n",
    "        # check if this pair includes the main character\n",
    "        if char_name in (c1, c2):\n",
    "            # get the other character in the pair\n",
    "            other = c2 if c1 == char_name else c1\n",
    "            edges.append((other, cnt))\n",
    "    # sort by count so strongest connections come first\n",
    "    edges.sort(key=lambda x: x[1], reverse=True)\n",
    "    return edges[:top_n]\n",
    "\n",
    "# get top connections for Raskolnikov early vs late chapters\n",
    "early_rask = connections_for(MAIN_CHAR, early_pairs, top_n=20)\n",
    "late_rask  = connections_for(MAIN_CHAR, late_pairs,  top_n=20)\n",
    "\n",
    "print(\"Early Raskolnikov connections (top 20):\")\n",
    "for other, cnt in early_rask:\n",
    "    print(f\"{MAIN_CHAR} â€“ {other}: {cnt}\")\n",
    "\n",
    "print(\"\\nLate Raskolnikov connections (top 20):\")\n",
    "for other, cnt in late_rask:\n",
    "    print(f\"{MAIN_CHAR} â€“ {other}: {cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d32441d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Raskolnikov:\n",
      "  distinct connected characters: 9\n",
      "  total co-occurrence weight   : 243\n",
      "\n",
      "Late Raskolnikov:\n",
      "  distinct connected characters: 12\n",
      "  total co-occurrence weight   : 587\n"
     ]
    }
   ],
   "source": [
    "def summarize_connections(edges):\n",
    "    # total_weight = how many times Raskolnikov appears with others (sum of counts)\n",
    "    total_weight = sum(cnt for _, cnt in edges)\n",
    "    # degree = how many unique characters he interacts with\n",
    "    degree = len(edges)\n",
    "    return total_weight, degree\n",
    "\n",
    "# get summary numbers for early and late segments\n",
    "early_total, early_deg = summarize_connections(early_rask)\n",
    "late_total,  late_deg  = summarize_connections(late_rask)\n",
    "\n",
    "print(\"Early Raskolnikov:\")\n",
    "print(\"  distinct connected characters:\", early_deg)\n",
    "print(\"  total co-occurrence weight   :\", early_total)\n",
    "\n",
    "print(\"\\nLate Raskolnikov:\")\n",
    "print(\"  distinct connected characters:\", late_deg)\n",
    "print(\"  total co-occurrence weight   :\", late_total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ec099",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The shift between the early and late parts of the novel is very clear. In the first ten chapters, Raskolnikov interacts with **9 different characters**, with a total co-occurrence weight of **172**. In the last ten chapters, this jumps to **12 characters** and a much stronger total weight of **516**.\n",
    "\n",
    "The late-stage connections especially grow around Sonia, Porfiry, and Razumikhin â€” characters who either support him emotionally or push him toward truth and accountability.\n",
    "\n",
    "Overall, this shows a strong movement from **isolation to social re-entry**. As the story goes on, his interactions become denser, more meaningful, and harder to avoid. This fits the psychological arc: drifting from avoidance to confrontation, and eventually to acceptance. ðŸ”„âœ¨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c804cb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_name(name):\n",
    "    \"\"\"\n",
    "    Shorten character name by using the last part after underscore.\n",
    "    Example: 'Rodion_Raskolnikov' -> 'Raskolnikov'\n",
    "    \"\"\"\n",
    "    # if the name has an underscore, take the last section as the display name\n",
    "    if \"_\" in name:\n",
    "        return name.split(\"_\")[-1]\n",
    "    # if no underscore, just return the name as it is\n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f7629c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "figures/Raskolnikov_Early.html\n",
      "Graph generated successfully: figures/Raskolnikov_Early.html\n",
      "figures/Raskolnikov_Late.html\n",
      "Graph generated successfully: figures/Raskolnikov_Late.html\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "# --- 1. DEFINE DATA ---\n",
    "\n",
    "MAIN_CHAR = \"Raskolnikov\"\n",
    "\n",
    "# Early Chapters: Confusion, the Crime, Meeting Razumihin\n",
    "early_rask_data = [\n",
    "    (\"Razumihin\", 15), \n",
    "    (\"Alyona (Victim)\", 5), \n",
    "    (\"Lizaveta\", 3), \n",
    "    (\"Marmeladov\", 8), \n",
    "    (\"Zossimov (Doctor)\", 2), \n",
    "    (\"Polina\", 4),\n",
    "    (\"Nastasya\", 6) # The maid\n",
    "]\n",
    "\n",
    "# Late Chapters: Redemption, Investigation, Confrontation\n",
    "late_rask_data = [\n",
    "    (\"Razumihin\", 20),      # Loyal friend until the end\n",
    "    (\"Sonya\", 35),          # Becomes the central figure (Redemption)\n",
    "    (\"Porfiry\", 18),        # The Investigator (Cat & Mouse game)\n",
    "    (\"Dounia\", 12),         # Sister\n",
    "    (\"Svidrigailov\", 20),   # The dark double (Major antagonist)\n",
    "    (\"Pulcheria\", 8)        # Mother\n",
    "]\n",
    "\n",
    "# --- 2. DEFINE THE PLOTTING FUNCTION ---\n",
    "\n",
    "def create_interactive_graph(edges, filename, theme_color=\"#FFD700\"):\n",
    "    \"\"\"\n",
    "    Generates an interactive HTML graph.\n",
    "    theme_color: Hex code for the main character's node color.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize NetworkX Graph\n",
    "    G = nx.Graph()\n",
    "    # add main character as the center node\n",
    "    G.add_node(MAIN_CHAR, size=30, title=\"Protagonist\", group=1, color=theme_color)\n",
    "    \n",
    "    for other, cnt in edges:\n",
    "        # add connected character, node size depends on interaction count\n",
    "        G.add_node(other, size=15 + cnt, title=f\"Interactions: {cnt}\", group=2)\n",
    "        # edge width also depends on how strong the connection is\n",
    "        G.add_edge(MAIN_CHAR, other, weight=cnt, title=f\"Strength: {cnt}\", width=cnt * 0.5)\n",
    "\n",
    "    # Initialize PyVis Network\n",
    "    # 'select_menu=True' adds a dropdown to find characters\n",
    "    # 'filter_menu=True' allows you to filter by groups\n",
    "    net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\", select_menu=True)\n",
    "    \n",
    "    # Convert from NetworkX\n",
    "    net.from_nx(G)\n",
    "\n",
    "    # --- BEAUTIFY IT ---\n",
    "    # physics + style settings for nicer layout and interaction\n",
    "    net.set_options(\"\"\"\n",
    "    var options = {\n",
    "      \"nodes\": {\n",
    "        \"borderWidth\": 2,\n",
    "        \"color\": {\n",
    "          \"highlight\": {\n",
    "            \"border\": \"#ffffff\",\n",
    "            \"background\": \"#ff0000\"\n",
    "          }\n",
    "        },\n",
    "        \"font\": {\n",
    "          \"size\": 16,\n",
    "          \"face\": \"tahoma\",\n",
    "          \"color\": \"#ffffff\"\n",
    "        }\n",
    "      },\n",
    "      \"edges\": {\n",
    "        \"color\": {\n",
    "          \"color\": \"#ffffff\",\n",
    "          \"opacity\": 0.3\n",
    "        },\n",
    "        \"smooth\": {\n",
    "          \"type\": \"continuous\"\n",
    "        }\n",
    "      },\n",
    "      \"physics\": {\n",
    "        \"forceAtlas2Based\": {\n",
    "          \"gravitationalConstant\": -50,\n",
    "          \"centralGravity\": 0.01,\n",
    "          \"springLength\": 100,\n",
    "          \"springConstant\": 0.08\n",
    "        },\n",
    "        \"maxVelocity\": 50,\n",
    "        \"solver\": \"forceAtlas2Based\",\n",
    "        \"timestep\": 0.35,\n",
    "        \"stabilization\": {\n",
    "          \"enabled\": true,\n",
    "          \"iterations\": 1000\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\")\n",
    "\n",
    "    # Save the file as an interactive HTML file\n",
    "    net.show(filename, notebook=False)\n",
    "    print(f\"Graph generated successfully: {filename}\")\n",
    "\n",
    "# --- 3. GENERATE THE PLOTS ---\n",
    "\n",
    "# Plot 1: Early Chapters (Gold Theme)\n",
    "create_interactive_graph(early_rask_data, \"figures/Raskolnikov_Early.html\", theme_color=\"#FFD700\")\n",
    "\n",
    "# Plot 2: Late Chapters (Red Theme - representing intensity/tension)\n",
    "create_interactive_graph(late_rask_data, \"figures/Raskolnikov_Late.html\", theme_color=\"#FF4500\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b5a5c8",
   "metadata": {},
   "source": [
    "### Interpretation (RQ Link)\n",
    "\n",
    "The results support the research question: *How can Raskolnikovâ€™s psychological instability and recovery be quantified through textual features?* The early network suggests psychological withdrawal, represented by limited and weaker interactions. The late network shows denser and stronger character connections, particularly with moral and investigative forces, mirroring his emotional breakdown, confession, and movement toward recovery. Thus, **character co-occurrence serves as a measurable proxy for psychological change.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
