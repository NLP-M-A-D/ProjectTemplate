{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f029625",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b668dfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CP characters: 1,211,652\n",
      "Chapters loaded: 1\n"
     ]
    }
   ],
   "source": [
    "def load_book(filepath: str) -> str:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    if 'CHAPTER I' in text:\n",
    "        start = text.find('CHAPTER I')\n",
    "        text = text[start:]\n",
    "    elif '*** START OF' in text:\n",
    "        start = text.find('*** START OF')\n",
    "        text = text[start + 100:]\n",
    "\n",
    "    if '*** END OF' in text:\n",
    "        end = text.find('*** END OF')\n",
    "        text = text[:end]\n",
    "    elif 'End of Project Gutenberg' in text:\n",
    "        end = text.find('End of Project Gutenberg')\n",
    "        text = text[:end]\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "CP_text = load_book('../data/Crime-punishment.txt')\n",
    "BK_text = load_book('../data/The-Brotherskaramazov.txt')\n",
    "\n",
    "print(f\"CP characters: {len(CP_text):,}\")\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "def split_chapters(text: str) -> List[str]:\n",
    "    chapters = re.split(r'\\bCHAPTER\\s+[IVXLCDM]+\\b', text)\n",
    "    chapters = [c.strip() for c in chapters if len(c.strip()) > 500]\n",
    "    return chapters\n",
    "\n",
    "CP_chapters = split_chapters(CP_text)\n",
    "\n",
    "print(\"Chapters loaded:\", len(CP_chapters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b6d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "def load_characters(path: str) -> Dict:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "charlib = load_characters(\"Character Library/Crime_punishment.json\")\n",
    "\n",
    "rask = charlib[\"Rodion_Raskolnikov\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a3c51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strong aliases: ['raskolnikov', 'rodion romanovich raskolnikov', 'rodion romanovich', 'rodion', 'rodya', 'rodka', 'rodenka']\n",
      "Weak descriptors: ['the student', 'the young man', 'the lodger', 'the murderer', 'the author of the article', 'the former student']\n"
     ]
    }
   ],
   "source": [
    "def normalize_aliases(alias_list):\n",
    "    strong = []\n",
    "    weak = []\n",
    "\n",
    "    for a in alias_list:\n",
    "        if isinstance(a, dict):\n",
    "            if a.get(\"tier\", 3) >= 3:\n",
    "                weak.append(a[\"text\"].lower())\n",
    "        else:\n",
    "            strong.append(a.lower())\n",
    "\n",
    "    return strong, weak\n",
    "\n",
    "RASK_ALIASES, RASK_WEAK = normalize_aliases(rask[\"aliases\"])\n",
    "\n",
    "print(\"Strong aliases:\", RASK_ALIASES)\n",
    "print(\"Weak descriptors:\", RASK_WEAK)\n",
    "RASK_ALIASES, RASK_WEAK = normalize_aliases(rask[\"aliases\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2181e3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Murder chapter index: 0\n"
     ]
    }
   ],
   "source": [
    "def detect_murder_chapter(chapters: List[str]) -> int:\n",
    "    murder_terms = [\n",
    "        \"axe\", \"hatchet\", \"blood\", \"pawnbroker\",\n",
    "        \"alyona\", \"lizaveta\", \"murdered\", \"killed\"\n",
    "    ]\n",
    "\n",
    "    for i, ch in enumerate(chapters):\n",
    "        text = ch.lower()\n",
    "        hits = sum(term in text for term in murder_terms)\n",
    "        if hits >= 3:\n",
    "            return i\n",
    "\n",
    "    raise RuntimeError(\"Murder chapter not detected.\")\n",
    "\n",
    "murder_idx = detect_murder_chapter(CP_chapters)\n",
    "print(\"Murder chapter index:\", murder_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9de1a3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def extract_character_passages(\n",
    "    chapters: List[str],\n",
    "    strong_aliases: List[str],\n",
    "    weak_aliases: List[str],\n",
    "    window: int = 1\n",
    "):\n",
    "    extracted = []\n",
    "\n",
    "    for chap_idx, chapter in enumerate(chapters):\n",
    "        sents = sent_tokenize(chapter)\n",
    "        for i, sent in enumerate(sents):\n",
    "            sent_lc = sent.lower()\n",
    "\n",
    "            strong_hit = any(a in sent_lc for a in strong_aliases)\n",
    "            weak_hit = any(w in sent_lc for w in weak_aliases)\n",
    "\n",
    "            if strong_hit or weak_hit:\n",
    "                start = max(0, i - window)\n",
    "                end = min(len(sents), i + window + 1)\n",
    "                context = \" \".join(sents[start:end])\n",
    "\n",
    "                extracted.append({\n",
    "                    \"chapter\": chap_idx,\n",
    "                    \"text\": context,\n",
    "                    \"strong_hit\": strong_hit\n",
    "                })\n",
    "\n",
    "    return extracted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afa627f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def extract_psych_features(text: str) -> dict:\n",
    "    doc = nlp(text)\n",
    "    words = [t.text.lower() for t in doc if t.is_alpha]\n",
    "    sents = sent_tokenize(text)\n",
    "\n",
    "    total_words = len(words) or 1\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    features[\"I_ratio\"] = words.count(\"i\") / total_words\n",
    "    features[\"negation_ratio\"] = sum(w in [\"no\",\"not\",\"never\",\"n't\"] for w in words) / total_words\n",
    "    features[\"modal_ratio\"] = sum(w in [\"must\",\"should\",\"ought\"] for w in words) / total_words\n",
    "\n",
    "    features[\"avg_sentence_length\"] = np.mean([len(word_tokenize(s)) for s in sents])\n",
    "    features[\"lexical_diversity\"] = len(set(words)) / total_words\n",
    "\n",
    "    pos_counts = Counter(t.pos_ for t in doc)\n",
    "    features[\"verb_ratio\"] = pos_counts[\"VERB\"] / total_words\n",
    "    features[\"adj_ratio\"] = pos_counts[\"ADJ\"] / total_words\n",
    "\n",
    "    sent = analyzer.polarity_scores(text)\n",
    "    features[\"sentiment_compound\"] = sent[\"compound\"]\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4db1cf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def extract_character_passages(\n",
    "    chapters,\n",
    "    strong_aliases,\n",
    "    weak_aliases,\n",
    "    window=1\n",
    "):\n",
    "    extracted = []\n",
    "\n",
    "    for chap_idx, chapter in enumerate(chapters):\n",
    "        sents = sent_tokenize(chapter)\n",
    "\n",
    "        for i, sent in enumerate(sents):\n",
    "            sent_lc = sent.lower()\n",
    "\n",
    "            strong_hit = any(a in sent_lc for a in strong_aliases)\n",
    "            weak_hit = any(w in sent_lc for w in weak_aliases)\n",
    "\n",
    "            if strong_hit or weak_hit:\n",
    "                start = max(0, i - window)\n",
    "                end = min(len(sents), i + window + 1)\n",
    "                context = \" \".join(sents[start:end])\n",
    "\n",
    "                extracted.append({\n",
    "                    \"chapter\": chap_idx,\n",
    "                    \"text\": context,\n",
    "                    \"strong_ref\": strong_hit   # ✅ always present\n",
    "                })\n",
    "\n",
    "    return extracted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82af05c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rask_passages = extract_character_passages(\n",
    "    CP_chapters,\n",
    "    RASK_ALIASES,\n",
    "    RASK_WEAK,\n",
    "    window=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61dbd1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rask_df shape: (969, 11)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "\n",
    "for p in rask_passages:\n",
    "    feats = extract_psych_features(p[\"text\"])\n",
    "    feats[\"chapter\"] = p[\"chapter\"]\n",
    "    feats[\"post_murder\"] = p[\"chapter\"] >= murder_idx\n",
    "    feats[\"strong_ref\"] = p[\"strong_ref\"]\n",
    "    rows.append(feats)\n",
    "\n",
    "rask_df = pd.DataFrame(rows)\n",
    "\n",
    "# Optional: filter to strong mentions only\n",
    "rask_df = rask_df[rask_df.strong_ref]\n",
    "\n",
    "print(\"rask_df shape:\", rask_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0c824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "FEATURES = [\n",
    "    \"I_ratio\",\n",
    "    \"negation_ratio\",\n",
    "    \"sentiment_compound\"\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(len(FEATURES), 1, figsize=(12, 9), sharex=True)\n",
    "\n",
    "for ax, feat in zip(axes, FEATURES):\n",
    "    sns.lineplot(\n",
    "        data=rask_df,\n",
    "        x=\"chapter\",\n",
    "        y=feat,\n",
    "        estimator=\"mean\",\n",
    "        ci=95,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    ax.axvline(murder_idx, color=\"red\", linestyle=\"--\", label=\"Murder\")\n",
    "    ax.set_ylabel(feat)\n",
    "    ax.legend()\n",
    "\n",
    "axes[-1].set_xlabel(\"Chapter\")\n",
    "plt.suptitle(\"Raskolnikov’s Psychological Shift Across Crime and Punishment\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45810398",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "sns.lineplot(\n",
    "    data=rask_df,\n",
    "    x=\"chapter\",\n",
    "    y=\"lexical_diversity\",\n",
    "    estimator=\"mean\",\n",
    "    ci=95,\n",
    "    label=\"Lexical diversity\",\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "sns.lineplot(\n",
    "    data=rask_df,\n",
    "    x=\"chapter\",\n",
    "    y=\"avg_sentence_length\",\n",
    "    estimator=\"mean\",\n",
    "    ci=95,\n",
    "    label=\"Avg sentence length\",\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.axvline(murder_idx, color=\"red\", linestyle=\"--\")\n",
    "ax.set_title(\"Cognitive Narrowing After the Murder\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34745f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df = rask_df.melt(\n",
    "    id_vars=\"post_murder\",\n",
    "    value_vars=[\n",
    "        \"I_ratio\",\n",
    "        \"negation_ratio\",\n",
    "        \"modal_ratio\",\n",
    "        \"sentiment_compound\"\n",
    "    ],\n",
    "    var_name=\"feature\",\n",
    "    value_name=\"value\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "sns.boxplot(\n",
    "    data=long_df,\n",
    "    x=\"feature\",\n",
    "    y=\"value\",\n",
    "    hue=\"post_murder\"\n",
    ")\n",
    "\n",
    "plt.title(\"Raskolnikov: Pre vs Post Murder Linguistic Profile\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc247f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "features_for_pca = [\n",
    "    \"I_ratio\",\n",
    "    \"negation_ratio\",\n",
    "    \"modal_ratio\",\n",
    "    \"lexical_diversity\",\n",
    "    \"sentiment_compound\"\n",
    "]\n",
    "\n",
    "X = rask_df[features_for_pca].dropna()\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "proj = pca.fit_transform(X_scaled)\n",
    "\n",
    "pca_df = pd.DataFrame(proj, columns=[\"PC1\", \"PC2\"])\n",
    "pca_df[\"post_murder\"] = rask_df.iloc[X.index][\"post_murder\"].values\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(\n",
    "    data=pca_df,\n",
    "    x=\"PC1\",\n",
    "    y=\"PC2\",\n",
    "    hue=\"post_murder\",\n",
    "    alpha=0.6\n",
    ")\n",
    "plt.title(\"Psychological State Space of Raskolnikov\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
